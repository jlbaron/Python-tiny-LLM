# Python-tiny-LLM
I find LLMs fascinating so I wanted to finetune one for personal use...locally. Many considerations in this repository are made for my 1 gpu desktop with 32 gb of ram and limited training time. Hopefully this is useful for anyone in the same situation.


<h2> Process </h2>
<p> I will document progress as I make it here.</p>

<h4> Sourcing model and data </h4>
<p> I fully acknowledge that I might have to come back here depending on the results, but for now I have found the following to be used:</p>
<p> Model: Pythia 410M       Dataset: CodeXGLUE Code Search (AdvTest) </p>
<p> Although I intend to use a bigger model I will start with the smallest for a sanity check </p>